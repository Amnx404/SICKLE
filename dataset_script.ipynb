{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "source_path = 'dataset'\n",
    "destination_path = 'dataset_50_per'\n",
    "\n",
    "def create_structure():\n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "    if not os.path.exists(os.path.join(destination_path, 'images')):\n",
    "        os.makedirs(os.path.join(destination_path, 'images'))\n",
    "    if not os.path.exists(os.path.join(destination_path, 'masks')):\n",
    "        shutil.copytree(os.path.join(source_path, 'masks'), os.path.join(destination_path, 'masks'))\n",
    "\n",
    "def process_npy_tif(satellite):\n",
    "    satellite_path = os.path.join(source_path, 'images', satellite)\n",
    "    dest_sat_path = os.path.join(destination_path, 'images', satellite)\n",
    "    \n",
    "    for data_type in ['npy', 'tif']:\n",
    "        data_path = os.path.join(satellite_path, data_type)\n",
    "        dest_data_path = os.path.join(dest_sat_path, data_type)\n",
    "        \n",
    "        if not os.path.exists(dest_data_path):\n",
    "            os.makedirs(dest_data_path)\n",
    "        \n",
    "        for folder in os.listdir(data_path):\n",
    "            folder_path = os.path.join(data_path, folder)\n",
    "            dest_folder_path = os.path.join(dest_data_path, folder)\n",
    "            \n",
    "            if data_type == 'npy':\n",
    "                npz_files = os.listdir(folder_path)\n",
    "                selected_files = random.sample(npz_files, len(npz_files) // 2)\n",
    "                os.makedirs(dest_folder_path)\n",
    "                for file in selected_files:\n",
    "                    shutil.copy2(os.path.join(folder_path, file), dest_folder_path)\n",
    "            else:\n",
    "                # 'tif' processing based on remaining 'npy' selection\n",
    "                corresponding_npy_path = os.path.join(dest_sat_path, 'npy', folder)\n",
    "                if os.path.exists(corresponding_npy_path):\n",
    "                    selected_folders = [f[:-4] for f in os.listdir(corresponding_npy_path)]  # assuming the names match minus .npz\n",
    "                    os.makedirs(dest_folder_path)\n",
    "                    for subfolder in selected_folders:\n",
    "                        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                        dest_subfolder_path = os.path.join(dest_folder_path, subfolder)\n",
    "                        if os.path.exists(subfolder_path):\n",
    "                            shutil.copytree(subfolder_path, dest_subfolder_path)\n",
    "\n",
    "create_structure()\n",
    "for satellite in ['L8', 'S2']:\n",
    "    process_npy_tif(satellite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'dataset'\n",
    "destination_path = 'dataset_20_per'\n",
    "\n",
    "def create_structure():\n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "    if not os.path.exists(os.path.join(destination_path, 'images')):\n",
    "        os.makedirs(os.path.join(destination_path, 'images'))\n",
    "    if not os.path.exists(os.path.join(destination_path, 'masks')):\n",
    "        shutil.copytree(os.path.join(source_path, 'masks'), os.path.join(destination_path, 'masks'))\n",
    "\n",
    "def process_npy_tif(satellite):\n",
    "    satellite_path = os.path.join(source_path, 'images', satellite)\n",
    "    dest_sat_path = os.path.join(destination_path, 'images', satellite)\n",
    "    \n",
    "    for data_type in ['npy', 'tif']:\n",
    "        data_path = os.path.join(satellite_path, data_type)\n",
    "        dest_data_path = os.path.join(dest_sat_path, data_type)\n",
    "        \n",
    "        if not os.path.exists(dest_data_path):\n",
    "            os.makedirs(dest_data_path)\n",
    "        \n",
    "        for folder in os.listdir(data_path):\n",
    "            folder_path = os.path.join(data_path, folder)\n",
    "            dest_folder_path = os.path.join(dest_data_path, folder)\n",
    "            \n",
    "            if data_type == 'npy':\n",
    "                npz_files = os.listdir(folder_path)\n",
    "                selected_files = random.sample(npz_files, len(npz_files) // 5)\n",
    "                os.makedirs(dest_folder_path)\n",
    "                for file in selected_files:\n",
    "                    shutil.copy2(os.path.join(folder_path, file), dest_folder_path)\n",
    "            else:\n",
    "                # 'tif' processing based on remaining 'npy' selection\n",
    "                corresponding_npy_path = os.path.join(dest_sat_path, 'npy', folder)\n",
    "                if os.path.exists(corresponding_npy_path):\n",
    "                    selected_folders = [f[:-4] for f in os.listdir(corresponding_npy_path)]  # assuming the names match minus .npz\n",
    "                    os.makedirs(dest_folder_path)\n",
    "                    for subfolder in selected_folders:\n",
    "                        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                        dest_subfolder_path = os.path.join(dest_folder_path, subfolder)\n",
    "                        if os.path.exists(subfolder_path):\n",
    "                            shutil.copytree(subfolder_path, dest_subfolder_path)\n",
    "\n",
    "create_structure()\n",
    "for satellite in ['L8', 'S2']:\n",
    "    process_npy_tif(satellite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shell script 'run_eval.sh' has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define datasets and tasks\n",
    "datasets = [\"dataset\", \"dataset_20_per\", \"dataset_50_per\"]\n",
    "tasks = [\"crop_type\"]  # Add more tasks here if needed\n",
    "base_run_path = \"runs/wacv_2024_seed0\"\n",
    "\n",
    "# Function to generate the shell commands\n",
    "def generate_shell_script(output_file=\"run_eval.sh\"):\n",
    "    # Ensure the output_results_verbose folder exists\n",
    "    os.makedirs(\"output_results_verbose\", exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"#!/bin/bash\\n\\n\")\n",
    "\n",
    "        # Loop over tasks and datasets to generate commands\n",
    "        for task in tasks:\n",
    "            for dataset in datasets:\n",
    "                for dataset_2 in datasets:\n",
    "                    base_path = f\"{base_run_path}/{task}\"\n",
    "\n",
    "                    # CONCAT case (fusion = 0)\n",
    "                    run_name_concat = f\"evaluate_test_seed0_batch_{task}_CONCAT_{dataset}_seed0_1_{dataset_2}\"\n",
    "                    best_path_concat = f\"{base_path}/CONCAT_{dataset}_seed0_1\"\n",
    "                    output_file_concat = f\"output_results_verbose/{run_name_concat}_output.txt\"\n",
    "                    f.write(f'python evaluate.py --data_dir ./{dataset_2} --satellite [S1,S2,L8] --task {task} --run_name \"{run_name_concat}\" --model utae --fusion 0 --best_path \"{best_path_concat}\" > {output_file_concat} 2>&1\\n')\n",
    "\n",
    "                    # CROSS_ALL_NORM_DROP case (fusion = 12)\n",
    "                    # run_name_norm_drop = f\"evaluate_test_seed0_batch_{task}_CROSS_ALL_NORM_DROP_{dataset}_seed0_1_{dataset_2}\"\n",
    "                    # best_path_norm_drop = f\"{base_path}/CROSS_ALL_NORM_DROP_{dataset}_seed0_1\"\n",
    "                    # output_file_norm_drop = f\"output_results_verbose/{run_name_norm_drop}_output.txt\"\n",
    "                    # f.write(f'python evaluate.py --data_dir ./{dataset_2} --satellite [S1,S2,L8] --task {task} --run_name \"{run_name_norm_drop}\" --model utae --fusion 12 --best_path \"{best_path_norm_drop}\" > {output_file_norm_drop} 2>&1\\n')\n",
    "\n",
    "                    # CROSS_ALL_NORM_BATCH_DROP case (fusion = 14)\n",
    "                    run_name_batch_drop = f\"evaluate_test_seed0_batch_{task}_CROSS_ALL_NORM_BATCH_DROP_{dataset}_seed0_1_{dataset_2}\"\n",
    "                    best_path_batch_drop = f\"{base_path}/CROSS_ALL_NORM_BATCH_DROP_{dataset}_seed0_1\"\n",
    "                    output_file_batch_drop = f\"output_results_verbose/{run_name_batch_drop}_output.txt\"\n",
    "                    f.write(f'python evaluate.py --data_dir ./{dataset_2} --satellite [S1,S2,L8] --task {task} --run_name \"{run_name_batch_drop}\" --model utae --fusion 14 --best_path \"{best_path_batch_drop}\" > {output_file_batch_drop} 2>&1\\n')\n",
    "\n",
    "        print(f\"Shell script '{output_file}' has been generated successfully.\")\n",
    "\n",
    "# Call the function to generate the script\n",
    "if __name__ == \"__main__\":\n",
    "    generate_shell_script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shell script 'run_eval_harvest.sh' has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define datasets and tasks\n",
    "# datasets = [\"dataset\", \"dataset_20_per\", \"dataset_50_per\"]\n",
    "tasks = [\"harvesting_date\"]  # Add more tasks here if needed\n",
    "base_run_path = \"runs/wacv_2024_seed\"\n",
    "\n",
    "# Function to generate the shell commands\n",
    "def generate_shell_script(output_file=\"run_eval_harvest.sh\"):\n",
    "    # Ensure the output_results_verbose folder exists\n",
    "    os.makedirs(\"output_results_verbose\", exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"#!/bin/bash\\n\\n\")\n",
    "\n",
    "        # Loop over tasks and datasets to generate commands\n",
    "        for seed in range(0,1):\n",
    "            for task in tasks:\n",
    "                for dataset_2 in ['dataset']: # test\n",
    "                    for dataset in ['dataset_50_per']:\n",
    "                        base_path = f\"{base_run_path+str(seed)}/{task}\"\n",
    "\n",
    "                        # CONCAT case (fusion = 0)\n",
    "                        run_name_concat = f\"evaluate_test_seed0_batch_{task}_CONCAT_{dataset}_seed{seed}_1_{dataset_2}_unet3d\"\n",
    "                        best_path_concat = f\"{base_path}/CONCAT_{dataset}_seed{seed}_1_unet3d\"\n",
    "                        output_file_concat = f\"output_results_verbose/{run_name_concat}_output.txt\"\n",
    "                        f.write(f'python evaluate.py --data_dir ./{dataset_2} --satellite [S1,S2,L8] --task {task} --run_name \"{run_name_concat}\" --model utae --fusion 0 --best_path \"{best_path_concat}\" > {output_file_concat} 2>&1\\n')\n",
    "\n",
    "                        # # CROSS_ALL_NORM_DROP case (fusion = 12)\n",
    "                        # run_name_norm_drop = f\"evaluate_test_seed0_batch_{task}_CROSS_ALL_NORM_DROP_{dataset}_seed{seed}_1_{dataset_2}_unet3d\"\n",
    "                        # best_path_norm_drop = f\"{base_path}/CROSS_ALL_NORM_DROP_{dataset}_seed{seed}_1_unet3d\"\n",
    "                        # output_file_norm_drop = f\"output_results_verbose/{run_name_norm_drop}_output.txt\"\n",
    "                        # f.write(f'python evaluate.py --data_dir ./{dataset_2} --satellite [S1,S2,L8] --task {task} --run_name \"{run_name_norm_drop}\" --model utae --fusion 12 --best_path \"{best_path_norm_drop}\" > {output_file_norm_drop} 2>&1\\n')\n",
    "\n",
    "                        # CROSS_ALL_NORM_BATCH_DROP case (fusion = 14)\n",
    "                        run_name_batch_drop = f\"evaluate_test_seed0_batch_{task}_CROSS_ALL_NORM_BATCH_DROP_{dataset}_seed{seed}_1_{dataset_2}_unet3d\"\n",
    "                        best_path_batch_drop = f\"{base_path}/CROSS_ALL_NORM_BATCH_DROP_{dataset}_seed{seed}_1\"\n",
    "                        output_file_batch_drop = f\"output_results_verbose/{run_name_batch_drop}_output.txt\"\n",
    "                        f.write(f'python evaluate.py --data_dir ./{dataset_2} --satellite [S1,S2,L8] --task {task} --run_name \"{run_name_batch_drop}\" --model unet3d --fusion 14 --best_path \"{best_path_batch_drop}\" > {output_file_batch_drop} 2>&1\\n')\n",
    "\n",
    "        print(f\"Shell script '{output_file}' has been generated successfully.\")\n",
    "\n",
    "# Call the function to generate the script\n",
    "if __name__ == \"__main__\":\n",
    "    generate_shell_script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sickle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
